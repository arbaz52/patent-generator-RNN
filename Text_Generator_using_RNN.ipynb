{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generator using RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arbaz52/patent-generator-RNN/blob/master/Text_Generator_using_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VGXQc8a4DYS",
        "colab_type": "text"
      },
      "source": [
        "# Connecting to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opg4c69z3hod",
        "colab_type": "code",
        "outputId": "2ab2262b-5c9f-4565-cb17-b6014ce1d1bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep9VPC4b4Hgs",
        "colab_type": "text"
      },
      "source": [
        "# Loading the Dataset - OLD - Do not use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqVJb8-x4MZS",
        "colab_type": "code",
        "outputId": "b596deb8-0d99-4af1-84ae-b29fbf1dcae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "drivepath = \"drive/My Drive/rnn/\"\n",
        "\n",
        "#loading the dataset from csv\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "df = pd.read_csv(drivepath+\"a95f913b.csv\", sep=\",\")\n",
        "\n",
        "\n",
        "df = df[['patent_title', 'patent_abstract']]\n",
        "\n",
        "def get_essay(df):\n",
        "  return df['patent_title'].fillna(\"\") + ' ' + df['patent_abstract']\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(filters='\"#$%&*+/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(get_essay(df))\n",
        "\n",
        "processed_df = df[\"patent_abstract\"].copy()\n",
        "processed_df[\"patent_abstract\"] = tokenizer.texts_to_sequences(get_essay(df))\n",
        "print(processed_df.head())\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "del df\n",
        "gc.collect()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0    \" A \"\"Barometer\"\" Neuron enhances stability in...\n",
            "1    \" This invention is a novel high-speed neural ...\n",
            "2    An optical information processor for use as a ...\n",
            "3    A method and system for intelligent control of...\n",
            "4    A method and system for intelligent control of...\n",
            "Name: patent_abstract, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eak4FMxfXkqt",
        "colab_type": "text"
      },
      "source": [
        "# Loading and pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfU07mmgXO9f",
        "colab_type": "code",
        "outputId": "03edc525-6eb4-4c2e-b986-67cf07d5a106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "import re\n",
        "\n",
        "\n",
        "def format_patent(patent):\n",
        "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
        "\n",
        "    # Add spaces around punctuation\n",
        "    patent = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', patent)\n",
        "\n",
        "    # Remove references to figures\n",
        "    patent = re.sub(r'\\((\\d+)\\)', r'', patent)\n",
        "\n",
        "    # Remove double spaces\n",
        "    patent = re.sub(r'\\s\\s', ' ', patent)\n",
        "    return patent\n",
        "\n",
        "\n",
        "drivepath = \"drive/My Drive/rnn/\"\n",
        "\n",
        "#loading the dataset from csv\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "df = pd.read_csv(drivepath+\"a95f913b.csv\", sep=\",\", nrows = 1500)\n",
        "\n",
        "abstracts = list(df['patent_abstract'])\n",
        "\n",
        "tokenizer = Tokenizer(filters='\"#$%&*+/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "\n",
        "formatted = []\n",
        "for abstract in abstracts:\n",
        "  formatted.append(format_patent(abstract))\n",
        "  \n",
        "tokenizer.fit_on_texts(formatted)\n",
        "sequences = tokenizer.texts_to_sequences(formatted)\n",
        "\n",
        "#to see whats in it\n",
        "\" \".join(tokenizer.index_word[n] for n in sequences[0])\n",
        "\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "del formatted, abstracts, df\n",
        "gc.collect()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdKRC0ghDjGL",
        "colab_type": "code",
        "outputId": "9f60725d-3451-496c-a8e7-d98220159258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import numpy as np\n",
        "features = []\n",
        "labels = []\n",
        "feature_length = 50\n",
        "for seq in sequences:\n",
        "  for i in range(feature_length, len(seq)):\n",
        "    extracted = seq[i - feature_length: i + 1]\n",
        "    features.append(extracted[:-1])\n",
        "    labels.append(extracted[-1])\n",
        "    \n",
        "features = np.array(features)\n",
        "labels = np.array(labels)\n",
        "print(features.shape, labels.shape)\n",
        "\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "del sequences\n",
        "gc.collect()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(137599, 50) (137599,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW3VMSCeFez6",
        "colab_type": "code",
        "outputId": "d57cde26-f808-4a3d-d9d1-55fa8deae9fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "idx_word = tokenizer.index_word\n",
        "num_words = len(idx_word) + 1\n",
        "\n",
        "print(\"Number of unique words: \", num_words)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique words:  9348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WedYwXGZJ7TK",
        "colab_type": "text"
      },
      "source": [
        "# One word encoding the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7ZBnKpeJ9zV",
        "colab_type": "code",
        "outputId": "05ca1f5f-dbbb-4541-b934-a93a995f189e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import numpy as np\n",
        "one_hot_labels = np.zeros((len(labels), num_words))\n",
        "print(one_hot_labels.shape)\n",
        "for i, idx in enumerate(labels):\n",
        "  one_hot_labels[i][idx] = 1\n",
        "  \n",
        "  \n",
        "print(one_hot_labels.shape)\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "del labels\n",
        "gc.collect()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(137599, 9348)\n",
            "(137599, 9348)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELJRFC5f_qPU",
        "colab_type": "text"
      },
      "source": [
        "# Loading the pretrained embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H5hn_Nc_of7",
        "colab_type": "code",
        "outputId": "447e6da3-4779-4dfb-b844-070f5987e32b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "glove=np.loadtxt(drivepath+'glove.6B.100d.txt', dtype='str', comments=None)\n",
        "words, vectors=glove[:,0],glove[:,1:].astype('float32')\n",
        "word_lookup={word:vector for word, vector in zip(words, vectors)}\n",
        "print(word_lookup['this'])\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "del glove, words\n",
        "gc.collect()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.57058   0.44183   0.70102  -0.41713  -0.34058   0.02339  -0.071537\n",
            "  0.48177  -0.013121  0.16834  -0.13389   0.040626  0.15827  -0.44342\n",
            " -0.019403 -0.009661 -0.046284  0.093228 -0.27331   0.2285    0.33089\n",
            " -0.36474   0.078741  0.3585    0.44757  -0.2299    0.18077  -0.6265\n",
            "  0.053852 -0.29154  -0.4256    0.62903   0.14393  -0.046004 -0.21007\n",
            "  0.48879  -0.057698  0.37431  -0.030075 -0.34494  -0.29702   0.15095\n",
            "  0.28248  -0.16578   0.076131 -0.093016  0.79365  -0.60489  -0.18874\n",
            " -1.0173    0.31962  -0.16344   0.54177   1.1725   -0.47875  -3.3842\n",
            " -0.081301 -0.3528    1.8372    0.44516  -0.52666   0.99786  -0.32178\n",
            "  0.033462  1.1783   -0.072905  0.39737   0.26166   0.33111  -0.35629\n",
            " -0.16558  -0.44382  -0.14183  -0.37976   0.28994  -0.029114 -0.35169\n",
            " -0.27694  -1.344     0.19555   0.16887   0.040237 -0.80212   0.23366\n",
            " -1.3837   -0.023132  0.085395 -0.74051  -0.073934 -0.58838  -0.085735\n",
            " -0.10525  -0.51571   0.15038  -0.16694  -0.16372  -0.22702  -0.66102\n",
            "  0.47197   0.37253 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3OvbVbYCYcC",
        "colab_type": "code",
        "outputId": "b141847c-b09e-4ec1-ab16-a10755387b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
        "\n",
        "for k in idx_word.keys():\n",
        "  vector = word_lookup.get(idx_word[k], None)\n",
        "  if vector is not None:\n",
        "    embedding_matrix[k, :] = vector\n",
        "    \n",
        "import gc\n",
        "gc.enable()\n",
        "del vectors\n",
        "gc.collect()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkxIVGY3rFXY",
        "colab_type": "code",
        "outputId": "af94848b-9304-416e-bf17-1aebfd3b644f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(embedding_matrix.shape, \n",
        "      one_hot_labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(9348, 100) (137599, 9348)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4B50hWHsevu",
        "colab_type": "text"
      },
      "source": [
        "# Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJH22wLssgwJ",
        "colab_type": "code",
        "outputId": "f3a6da55-945f-46ef-fa3d-c951591dd896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, CuDNNLSTM, Dense, Dropout, Masking, Embedding\n",
        "\n",
        "training_length = feature_length\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim = num_words,\n",
        "                   input_length = training_length,\n",
        "                   output_dim = 100,\n",
        "                   weights = [embedding_matrix],\n",
        "                   trainable = True,\n",
        "#                   mask_zero = True\n",
        "                   ))\n",
        "\n",
        "#model.add(Masking(mask_value = 0.0))\n",
        "\n",
        "model.add(LSTM(128,\n",
        "                    return_sequences = False,\n",
        "                    dropout = 0.1, recurrent_dropout = 0.1))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(num_words, activation = 'softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer = 'adadelta',\n",
        "             loss = 'categorical_crossentropy',\n",
        "             metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 50, 100)           934800    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 9348)              1205892   \n",
            "=================================================================\n",
            "Total params: 2,274,452\n",
            "Trainable params: 2,274,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgUhtOUhvZrF",
        "colab_type": "text"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQQwpFxuvvMF",
        "colab_type": "code",
        "outputId": "18cc8385-a435-4bac-f6c4-6537c2c28962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(features, one_hot_labels)\n",
        "\n",
        "print(x_train.shape, y_train.shape,\n",
        "     x_test.shape, y_test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(103199, 50) (103199, 9348) (34400, 50) (34400, 9348)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oriRteljFg-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Create callbacks\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
        "             ModelCheckpoint(drivepath+'rnn_model.h5', save_best_only=True, \n",
        "                             save_weights_only=False)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0JEXciJvZQ_",
        "colab_type": "code",
        "outputId": "e595cccc-080a-4760-97d7-f29b2bf09aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "hist = model.fit(x_train, y_train,\n",
        "                  batch_size = 2048, epochs = 150, callbacks=callbacks,\n",
        "                 shuffle = True,\n",
        "                  validation_data = (x_test, y_test))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 103199 samples, validate on 34400 samples\n",
            "Epoch 1/150\n",
            "103199/103199 [==============================] - 17s 169us/step - loss: 7.2156 - acc: 0.0624 - val_loss: 6.3598 - val_acc: 0.0312\n",
            "Epoch 2/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 6.3407 - acc: 0.0744 - val_loss: 6.2670 - val_acc: 0.0312\n",
            "Epoch 3/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 6.2640 - acc: 0.0778 - val_loss: 6.1693 - val_acc: 0.0862\n",
            "Epoch 4/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 6.2051 - acc: 0.0883 - val_loss: 6.1027 - val_acc: 0.1013\n",
            "Epoch 5/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 6.1344 - acc: 0.1007 - val_loss: 6.0344 - val_acc: 0.1015\n",
            "Epoch 6/150\n",
            "103199/103199 [==============================] - 16s 151us/step - loss: 6.0821 - acc: 0.1066 - val_loss: 5.9873 - val_acc: 0.1170\n",
            "Epoch 7/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 6.0387 - acc: 0.1135 - val_loss: 5.9570 - val_acc: 0.1185\n",
            "Epoch 8/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.9956 - acc: 0.1166 - val_loss: 5.9020 - val_acc: 0.1227\n",
            "Epoch 9/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 5.9400 - acc: 0.1247 - val_loss: 5.8424 - val_acc: 0.1416\n",
            "Epoch 10/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.8852 - acc: 0.1325 - val_loss: 5.7833 - val_acc: 0.1458\n",
            "Epoch 11/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.8329 - acc: 0.1376 - val_loss: 5.7380 - val_acc: 0.1482\n",
            "Epoch 12/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.7958 - acc: 0.1413 - val_loss: 5.7039 - val_acc: 0.1460\n",
            "Epoch 13/150\n",
            "103199/103199 [==============================] - 17s 163us/step - loss: 5.7610 - acc: 0.1428 - val_loss: 5.6707 - val_acc: 0.1527\n",
            "Epoch 14/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 5.7307 - acc: 0.1464 - val_loss: 5.6619 - val_acc: 0.1542\n",
            "Epoch 15/150\n",
            "103199/103199 [==============================] - 17s 162us/step - loss: 5.7022 - acc: 0.1484 - val_loss: 5.6139 - val_acc: 0.1488\n",
            "Epoch 16/150\n",
            "103199/103199 [==============================] - 17s 161us/step - loss: 5.6803 - acc: 0.1513 - val_loss: 5.6026 - val_acc: 0.1605\n",
            "Epoch 17/150\n",
            "103199/103199 [==============================] - 17s 161us/step - loss: 5.6535 - acc: 0.1528 - val_loss: 5.5738 - val_acc: 0.1618\n",
            "Epoch 18/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 5.6318 - acc: 0.1538 - val_loss: 5.5583 - val_acc: 0.1558\n",
            "Epoch 19/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.6133 - acc: 0.1553 - val_loss: 5.5435 - val_acc: 0.1614\n",
            "Epoch 20/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.5925 - acc: 0.1566 - val_loss: 5.5195 - val_acc: 0.1648\n",
            "Epoch 21/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.5791 - acc: 0.1581 - val_loss: 5.5102 - val_acc: 0.1655\n",
            "Epoch 22/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 5.5575 - acc: 0.1588 - val_loss: 5.5007 - val_acc: 0.1643\n",
            "Epoch 23/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 5.5429 - acc: 0.1586 - val_loss: 5.4750 - val_acc: 0.1648\n",
            "Epoch 24/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 5.5266 - acc: 0.1603 - val_loss: 5.4606 - val_acc: 0.1701\n",
            "Epoch 25/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.5123 - acc: 0.1619 - val_loss: 5.4453 - val_acc: 0.1695\n",
            "Epoch 26/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 5.4995 - acc: 0.1632 - val_loss: 5.4397 - val_acc: 0.1658\n",
            "Epoch 27/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.4849 - acc: 0.1629 - val_loss: 5.4221 - val_acc: 0.1711\n",
            "Epoch 28/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 5.4736 - acc: 0.1637 - val_loss: 5.4131 - val_acc: 0.1653\n",
            "Epoch 29/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.4596 - acc: 0.1643 - val_loss: 5.3935 - val_acc: 0.1738\n",
            "Epoch 30/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 5.4485 - acc: 0.1647 - val_loss: 5.3855 - val_acc: 0.1736\n",
            "Epoch 31/150\n",
            "103199/103199 [==============================] - 17s 162us/step - loss: 5.4367 - acc: 0.1651 - val_loss: 5.3867 - val_acc: 0.1721\n",
            "Epoch 32/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.4296 - acc: 0.1659 - val_loss: 5.3683 - val_acc: 0.1731\n",
            "Epoch 33/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.4160 - acc: 0.1665 - val_loss: 5.3691 - val_acc: 0.1696\n",
            "Epoch 34/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.4023 - acc: 0.1676 - val_loss: 5.3474 - val_acc: 0.1780\n",
            "Epoch 35/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 5.3983 - acc: 0.1681 - val_loss: 5.3393 - val_acc: 0.1782\n",
            "Epoch 36/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.3785 - acc: 0.1690 - val_loss: 5.3357 - val_acc: 0.1777\n",
            "Epoch 37/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.3740 - acc: 0.1677 - val_loss: 5.3218 - val_acc: 0.1782\n",
            "Epoch 38/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.3655 - acc: 0.1690 - val_loss: 5.3134 - val_acc: 0.1797\n",
            "Epoch 39/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.3566 - acc: 0.1703 - val_loss: 5.3077 - val_acc: 0.1814\n",
            "Epoch 40/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.3461 - acc: 0.1710 - val_loss: 5.3043 - val_acc: 0.1795\n",
            "Epoch 41/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.3330 - acc: 0.1719 - val_loss: 5.2882 - val_acc: 0.1825\n",
            "Epoch 42/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.3273 - acc: 0.1717 - val_loss: 5.2808 - val_acc: 0.1808\n",
            "Epoch 43/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.3214 - acc: 0.1716 - val_loss: 5.2686 - val_acc: 0.1821\n",
            "Epoch 44/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.3070 - acc: 0.1726 - val_loss: 5.2648 - val_acc: 0.1816\n",
            "Epoch 45/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.2976 - acc: 0.1727 - val_loss: 5.2584 - val_acc: 0.1828\n",
            "Epoch 46/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.2907 - acc: 0.1728 - val_loss: 5.2528 - val_acc: 0.1844\n",
            "Epoch 47/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.2787 - acc: 0.1743 - val_loss: 5.2429 - val_acc: 0.1824\n",
            "Epoch 48/150\n",
            "103199/103199 [==============================] - 16s 152us/step - loss: 5.2770 - acc: 0.1739 - val_loss: 5.2381 - val_acc: 0.1824\n",
            "Epoch 49/150\n",
            "103199/103199 [==============================] - 16s 153us/step - loss: 5.2673 - acc: 0.1738 - val_loss: 5.2251 - val_acc: 0.1863\n",
            "Epoch 50/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.2566 - acc: 0.1753 - val_loss: 5.2225 - val_acc: 0.1778\n",
            "Epoch 51/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.2513 - acc: 0.1746 - val_loss: 5.2142 - val_acc: 0.1850\n",
            "Epoch 52/150\n",
            "103199/103199 [==============================] - 16s 151us/step - loss: 5.2421 - acc: 0.1759 - val_loss: 5.2140 - val_acc: 0.1875\n",
            "Epoch 53/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.2333 - acc: 0.1767 - val_loss: 5.1917 - val_acc: 0.1901\n",
            "Epoch 54/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.2236 - acc: 0.1775 - val_loss: 5.1916 - val_acc: 0.1839\n",
            "Epoch 55/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.2164 - acc: 0.1773 - val_loss: 5.1767 - val_acc: 0.1893\n",
            "Epoch 56/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.2101 - acc: 0.1788 - val_loss: 5.1746 - val_acc: 0.1873\n",
            "Epoch 57/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.2021 - acc: 0.1784 - val_loss: 5.1649 - val_acc: 0.1888\n",
            "Epoch 58/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.1945 - acc: 0.1795 - val_loss: 5.1581 - val_acc: 0.1861\n",
            "Epoch 59/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.1847 - acc: 0.1792 - val_loss: 5.1488 - val_acc: 0.1918\n",
            "Epoch 60/150\n",
            "103199/103199 [==============================] - 16s 153us/step - loss: 5.1727 - acc: 0.1797 - val_loss: 5.1488 - val_acc: 0.1867\n",
            "Epoch 61/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.1695 - acc: 0.1815 - val_loss: 5.1411 - val_acc: 0.1962\n",
            "Epoch 62/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.1636 - acc: 0.1814 - val_loss: 5.1352 - val_acc: 0.1974\n",
            "Epoch 63/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.1553 - acc: 0.1813 - val_loss: 5.1252 - val_acc: 0.1923\n",
            "Epoch 64/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.1469 - acc: 0.1822 - val_loss: 5.1269 - val_acc: 0.1950\n",
            "Epoch 65/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.1369 - acc: 0.1812 - val_loss: 5.1168 - val_acc: 0.1939\n",
            "Epoch 66/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.1352 - acc: 0.1821 - val_loss: 5.1150 - val_acc: 0.1941\n",
            "Epoch 67/150\n",
            "103199/103199 [==============================] - 16s 152us/step - loss: 5.1286 - acc: 0.1821 - val_loss: 5.1017 - val_acc: 0.1954\n",
            "Epoch 68/150\n",
            "103199/103199 [==============================] - 17s 161us/step - loss: 5.1203 - acc: 0.1828 - val_loss: 5.1026 - val_acc: 0.1985\n",
            "Epoch 69/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.1094 - acc: 0.1845 - val_loss: 5.0956 - val_acc: 0.1977\n",
            "Epoch 70/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.1083 - acc: 0.1836 - val_loss: 5.0827 - val_acc: 0.2002\n",
            "Epoch 71/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.0999 - acc: 0.1843 - val_loss: 5.0848 - val_acc: 0.1953\n",
            "Epoch 72/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.0954 - acc: 0.1841 - val_loss: 5.0775 - val_acc: 0.1979\n",
            "Epoch 73/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.0884 - acc: 0.1863 - val_loss: 5.0715 - val_acc: 0.1953\n",
            "Epoch 74/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.0769 - acc: 0.1859 - val_loss: 5.0686 - val_acc: 0.1986\n",
            "Epoch 75/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.0727 - acc: 0.1857 - val_loss: 5.0654 - val_acc: 0.1949\n",
            "Epoch 76/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 5.0715 - acc: 0.1850 - val_loss: 5.0604 - val_acc: 0.2006\n",
            "Epoch 77/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.0608 - acc: 0.1870 - val_loss: 5.0527 - val_acc: 0.2038\n",
            "Epoch 78/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.0558 - acc: 0.1863 - val_loss: 5.0447 - val_acc: 0.1990\n",
            "Epoch 79/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 5.0541 - acc: 0.1881 - val_loss: 5.0421 - val_acc: 0.2032\n",
            "Epoch 80/150\n",
            "103199/103199 [==============================] - 16s 152us/step - loss: 5.0464 - acc: 0.1872 - val_loss: 5.0479 - val_acc: 0.1965\n",
            "Epoch 81/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 5.0374 - acc: 0.1873 - val_loss: 5.0286 - val_acc: 0.2036\n",
            "Epoch 82/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.0348 - acc: 0.1874 - val_loss: 5.0260 - val_acc: 0.2041\n",
            "Epoch 83/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.0347 - acc: 0.1882 - val_loss: 5.0275 - val_acc: 0.2001\n",
            "Epoch 84/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 5.0269 - acc: 0.1876 - val_loss: 5.0179 - val_acc: 0.2062\n",
            "Epoch 85/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 5.0225 - acc: 0.1884 - val_loss: 5.0137 - val_acc: 0.2060\n",
            "Epoch 86/150\n",
            "103199/103199 [==============================] - 16s 153us/step - loss: 5.0119 - acc: 0.1893 - val_loss: 5.0080 - val_acc: 0.2071\n",
            "Epoch 87/150\n",
            "103199/103199 [==============================] - 17s 162us/step - loss: 5.0090 - acc: 0.1902 - val_loss: 5.0063 - val_acc: 0.2022\n",
            "Epoch 88/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 5.0035 - acc: 0.1899 - val_loss: 5.0152 - val_acc: 0.1898\n",
            "Epoch 89/150\n",
            "103199/103199 [==============================] - 16s 153us/step - loss: 4.9999 - acc: 0.1900 - val_loss: 5.0060 - val_acc: 0.2018\n",
            "Epoch 90/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.9914 - acc: 0.1903 - val_loss: 4.9979 - val_acc: 0.2069\n",
            "Epoch 91/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 4.9857 - acc: 0.1906 - val_loss: 4.9897 - val_acc: 0.2060\n",
            "Epoch 92/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 4.9799 - acc: 0.1901 - val_loss: 4.9952 - val_acc: 0.2038\n",
            "Epoch 93/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.9743 - acc: 0.1894 - val_loss: 4.9883 - val_acc: 0.2042\n",
            "Epoch 94/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.9745 - acc: 0.1910 - val_loss: 4.9895 - val_acc: 0.2028\n",
            "Epoch 95/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.9735 - acc: 0.1921 - val_loss: 4.9739 - val_acc: 0.2067\n",
            "Epoch 96/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 4.9673 - acc: 0.1903 - val_loss: 4.9713 - val_acc: 0.2051\n",
            "Epoch 97/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.9581 - acc: 0.1921 - val_loss: 4.9646 - val_acc: 0.2103\n",
            "Epoch 98/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.9561 - acc: 0.1924 - val_loss: 4.9653 - val_acc: 0.2059\n",
            "Epoch 99/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.9523 - acc: 0.1925 - val_loss: 4.9639 - val_acc: 0.2076\n",
            "Epoch 100/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.9433 - acc: 0.1925 - val_loss: 4.9593 - val_acc: 0.2124\n",
            "Epoch 101/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.9432 - acc: 0.1919 - val_loss: 4.9587 - val_acc: 0.2131\n",
            "Epoch 102/150\n",
            "103199/103199 [==============================] - 16s 153us/step - loss: 4.9308 - acc: 0.1942 - val_loss: 4.9510 - val_acc: 0.2135\n",
            "Epoch 103/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.9342 - acc: 0.1928 - val_loss: 4.9482 - val_acc: 0.2099\n",
            "Epoch 104/150\n",
            "103199/103199 [==============================] - 16s 153us/step - loss: 4.9304 - acc: 0.1931 - val_loss: 4.9567 - val_acc: 0.2072\n",
            "Epoch 105/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.9243 - acc: 0.1924 - val_loss: 4.9548 - val_acc: 0.2090\n",
            "Epoch 106/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.9194 - acc: 0.1939 - val_loss: 4.9414 - val_acc: 0.2097\n",
            "Epoch 107/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.9185 - acc: 0.1940 - val_loss: 4.9384 - val_acc: 0.2117\n",
            "Epoch 108/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.9121 - acc: 0.1935 - val_loss: 4.9349 - val_acc: 0.2115\n",
            "Epoch 109/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.9092 - acc: 0.1943 - val_loss: 4.9302 - val_acc: 0.2101\n",
            "Epoch 110/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 4.9066 - acc: 0.1950 - val_loss: 4.9225 - val_acc: 0.2132\n",
            "Epoch 111/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.9070 - acc: 0.1943 - val_loss: 4.9313 - val_acc: 0.2033\n",
            "Epoch 112/150\n",
            "103199/103199 [==============================] - 16s 153us/step - loss: 4.8969 - acc: 0.1953 - val_loss: 4.9286 - val_acc: 0.2093\n",
            "Epoch 113/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8893 - acc: 0.1952 - val_loss: 4.9211 - val_acc: 0.2129\n",
            "Epoch 114/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8919 - acc: 0.1950 - val_loss: 4.9155 - val_acc: 0.2107\n",
            "Epoch 115/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.8827 - acc: 0.1952 - val_loss: 4.9290 - val_acc: 0.2075\n",
            "Epoch 116/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8796 - acc: 0.1950 - val_loss: 4.9163 - val_acc: 0.2140\n",
            "Epoch 117/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 4.8734 - acc: 0.1952 - val_loss: 4.8997 - val_acc: 0.2156\n",
            "Epoch 118/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 4.8748 - acc: 0.1980 - val_loss: 4.9097 - val_acc: 0.2104\n",
            "Epoch 119/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.8744 - acc: 0.1950 - val_loss: 4.8971 - val_acc: 0.2127\n",
            "Epoch 120/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.8631 - acc: 0.1968 - val_loss: 4.9074 - val_acc: 0.2123\n",
            "Epoch 121/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.8626 - acc: 0.1959 - val_loss: 4.9011 - val_acc: 0.2064\n",
            "Epoch 122/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8615 - acc: 0.1968 - val_loss: 4.8958 - val_acc: 0.2132\n",
            "Epoch 123/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.8548 - acc: 0.1967 - val_loss: 4.8958 - val_acc: 0.2147\n",
            "Epoch 124/150\n",
            "103199/103199 [==============================] - 17s 160us/step - loss: 4.8547 - acc: 0.1970 - val_loss: 4.9105 - val_acc: 0.2067\n",
            "Epoch 125/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.8513 - acc: 0.1973 - val_loss: 4.8852 - val_acc: 0.2147\n",
            "Epoch 126/150\n",
            "103199/103199 [==============================] - 16s 157us/step - loss: 4.8452 - acc: 0.1977 - val_loss: 4.8852 - val_acc: 0.2140\n",
            "Epoch 127/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 4.8460 - acc: 0.1974 - val_loss: 4.8849 - val_acc: 0.2147\n",
            "Epoch 128/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.8363 - acc: 0.1974 - val_loss: 4.8761 - val_acc: 0.2141\n",
            "Epoch 129/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8376 - acc: 0.1978 - val_loss: 4.8804 - val_acc: 0.2122\n",
            "Epoch 130/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8353 - acc: 0.1981 - val_loss: 4.8724 - val_acc: 0.2162\n",
            "Epoch 131/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8321 - acc: 0.1985 - val_loss: 4.8765 - val_acc: 0.2165\n",
            "Epoch 132/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8234 - acc: 0.1981 - val_loss: 4.8721 - val_acc: 0.2135\n",
            "Epoch 133/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 4.8272 - acc: 0.1975 - val_loss: 4.8720 - val_acc: 0.2135\n",
            "Epoch 134/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 4.8198 - acc: 0.1985 - val_loss: 4.8615 - val_acc: 0.2176\n",
            "Epoch 135/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 4.8199 - acc: 0.2001 - val_loss: 4.8591 - val_acc: 0.2163\n",
            "Epoch 136/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8094 - acc: 0.1992 - val_loss: 4.8586 - val_acc: 0.2200\n",
            "Epoch 137/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 4.8094 - acc: 0.1998 - val_loss: 4.8613 - val_acc: 0.2165\n",
            "Epoch 138/150\n",
            "103199/103199 [==============================] - 16s 158us/step - loss: 4.8090 - acc: 0.1988 - val_loss: 4.8466 - val_acc: 0.2189\n",
            "Epoch 139/150\n",
            "103199/103199 [==============================] - 16s 154us/step - loss: 4.8017 - acc: 0.2000 - val_loss: 4.8607 - val_acc: 0.2140\n",
            "Epoch 140/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.8028 - acc: 0.1986 - val_loss: 4.8507 - val_acc: 0.2159\n",
            "Epoch 141/150\n",
            "103199/103199 [==============================] - 16s 156us/step - loss: 4.8020 - acc: 0.1998 - val_loss: 4.8510 - val_acc: 0.2187\n",
            "Epoch 142/150\n",
            "103199/103199 [==============================] - 16s 155us/step - loss: 4.7976 - acc: 0.1990 - val_loss: 4.8493 - val_acc: 0.2105\n",
            "Epoch 143/150\n",
            "103199/103199 [==============================] - 16s 159us/step - loss: 4.7931 - acc: 0.2010 - val_loss: 4.8479 - val_acc: 0.2112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkEUPJO72S-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(drivepath+\"rnn-text-generator-added-additional-dense-150-epochs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y42Ljco_EbN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(drivepath+\"rnn-text-generator-150-epochs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OurnnXb2YuF",
        "colab_type": "text"
      },
      "source": [
        "# Testing the trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4RhCarP2ubl",
        "colab_type": "code",
        "outputId": "a00b1e3d-9a5f-430d-c500-3d7a926588e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        }
      },
      "source": [
        "\"\"\"\n",
        "print(x_train[122][:25].shape,  x_train[154][:25].shape)\n",
        "print()\n",
        "paragraph = \" \".join(idx_word[k] for k in (np.concatenate([ x_train[123][:25], x_train[152][:25]])))\n",
        "\"\"\"\n",
        "paragraph = \"The thing with the people is that they do not know what to do or what to say at the right moment , we have been working on this new technology that would help the humans to generate new technologies in a matter of minutes , but unfortunately we do not have enough\"\n",
        "print(paragraph)\n",
        "n_words_to_generate = 50\n",
        "for i in range(n_words_to_generate):\n",
        "  print(\"working on: \" + str(tokenizer.texts_to_sequences([paragraph])[0][-50:]))\n",
        "  seq = tokenizer.texts_to_sequences([paragraph])[0][-50:]\n",
        "  seq = np.array([seq])\n",
        "  next_prob = model.predict(seq)[0]\n",
        "  ind = np.argmax(next_prob)\n",
        "  word = idx_word[ind]\n",
        "  paragraph += \" \" + word\n",
        "  \n",
        "print(paragraph)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The thing with the people is that they do not know what to do or what to say at the right moment , we have been working on this new technology that would help the humans to generate new technologies in a matter of minutes , but unfortunately we do not have enough\n",
            "working on: [1, 19, 1, 8576, 10, 28, 1041, 1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478]\n",
            "working on: [19, 1, 8576, 10, 28, 1041, 1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3]\n",
            "working on: [1, 8576, 10, 28, 1041, 1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1]\n",
            "working on: [8576, 10, 28, 1041, 1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9]\n",
            "working on: [10, 28, 1041, 1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8]\n",
            "working on: [28, 1041, 1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10]\n",
            "working on: [1041, 1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47]\n",
            "working on: [1553, 222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7]\n",
            "working on: [222, 3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1]\n",
            "working on: [3575, 7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9]\n",
            "working on: [7, 1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8]\n",
            "working on: [1553, 18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4]\n",
            "working on: [18, 3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1]\n",
            "working on: [3575, 7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9]\n",
            "working on: [7, 6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8]\n",
            "working on: [6360, 33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10]\n",
            "working on: [33, 1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47]\n",
            "working on: [1, 1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7]\n",
            "working on: [1187, 3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1]\n",
            "working on: [3322, 5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9]\n",
            "working on: [5, 2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8]\n",
            "working on: [2972, 209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4]\n",
            "working on: [209, 249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1]\n",
            "working on: [249, 2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9]\n",
            "working on: [2611, 26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8]\n",
            "working on: [26, 81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10]\n",
            "working on: [81, 232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47]\n",
            "working on: [232, 807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7]\n",
            "working on: [807, 28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1]\n",
            "working on: [28, 1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9]\n",
            "working on: [1258, 3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8]\n",
            "working on: [3147, 1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4]\n",
            "working on: [1, 5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1]\n",
            "working on: [5587, 7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9]\n",
            "working on: [7, 106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8]\n",
            "working on: [106, 232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10]\n",
            "working on: [232, 1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47]\n",
            "working on: [1711, 12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7]\n",
            "working on: [12, 2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1]\n",
            "working on: [2, 1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9]\n",
            "working on: [1911, 3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8]\n",
            "working on: [3, 6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4]\n",
            "working on: [6557, 5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1]\n",
            "working on: [5, 599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9]\n",
            "working on: [599, 2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8]\n",
            "working on: [2972, 1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10]\n",
            "working on: [1553, 222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47]\n",
            "working on: [222, 209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7]\n",
            "working on: [209, 3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1]\n",
            "working on: [3478, 3, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9, 8, 4, 1, 9, 8, 10, 47, 7, 1, 9]\n",
            "The thing with the people is that they do not know what to do or what to say at the right moment , we have been working on this new technology that would help the humans to generate new technologies in a matter of minutes , but unfortunately we do not have enough of the neural network is used to the neural network . the neural network is used to the neural network . the neural network is used to the neural network . the neural network is used to the neural network . the neural network is used to the neural network\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}